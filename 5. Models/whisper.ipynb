{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7e8c37",
   "metadata": {},
   "source": [
    "Please note: this notebook was designed to be used in a Sagemaker environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37325c-78f6-4ed7-ad2f-44f0e0b34780",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de0e07-1f96-4872-9cb7-403ecf37c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers.utils import logging\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import time\n",
    "import boto3\n",
    "from os.path import isfile, join\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import ffmpeg\n",
    "import csv\n",
    "import argparse\n",
    "import string\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58930196-c99a-4af0-b991-dc2fe7c5953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'your/bucket/name'\n",
    "script_dir = join(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b8e9ab-e2f6-48ae-9eb7-5d61e88db204",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "transformers_logger = logging.get_logger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9811b5b-acb8-4207-a955-20696e0ebd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_client():\n",
    "    return boto3.client('s3')\n",
    "\n",
    "\n",
    "def list_s3_files_folder(folder_name):\n",
    "    if not folder_name.endswith('/'):\n",
    "        folder_name += '/'\n",
    "\n",
    "    keys = []\n",
    "    continuation_token = None\n",
    "\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(\n",
    "                Bucket=BUCKET_NAME,\n",
    "                Prefix=folder_name,\n",
    "                ContinuationToken=continuation_token\n",
    "            )\n",
    "        else:\n",
    "            response = s3.list_objects_v2(\n",
    "                Bucket=BUCKET_NAME,\n",
    "                Prefix=folder_name\n",
    "            )\n",
    "\n",
    "        contents = response.get('Contents', [])\n",
    "        keys.extend(\n",
    "            obj['Key'] for obj in contents if obj['Key'].endswith('.wav')\n",
    "        )\n",
    "\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "def get_file_from_folder(folder_name, file_name):\n",
    "    if not folder_name.endswith('/'):\n",
    "        folder_name += '/'\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    s3.download_fileobj(BUCKET_NAME, file_name, buffer)\n",
    "\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "def get_reference(folder_name):\n",
    "    if not folder_name.endswith('/'):\n",
    "        folder_name += '/'\n",
    "\n",
    "    x = folder_name.rstrip('/').split('.')[0]\n",
    "    expected_filename = f\"{x}_reference.stm\"\n",
    "    expected_key = folder_name + expected_filename\n",
    "\n",
    "    if (folder_name == 'testing_subset/'):\n",
    "        expected_key = \"1. nonnative-read/1_reference.stm\"\n",
    "        folder_name = \"1. nonnative-read/\"\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=folder_name)\n",
    "\n",
    "    for obj in response.get('Contents', []):\n",
    "        if obj['Key'] == expected_key:\n",
    "            buffer = BytesIO()\n",
    "            s3.download_fileobj(BUCKET_NAME, expected_key, buffer)\n",
    "            stm_text = buffer.getvalue().decode(\"utf-8\")\n",
    "            return stm_text\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def list_folders():\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME, Delimiter='/')\n",
    "    return [p['Prefix'].rstrip('/') for p in response.get('CommonPrefixes', [])]\n",
    "\n",
    "\n",
    "def get_duration(stm_text, s3_key):\n",
    "    # Extract the filename without extension from the full S3 key\n",
    "    clip_base = os.path.splitext(os.path.basename(s3_key))[0]\n",
    "\n",
    "    for line in stm_text.splitlines():\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 5:\n",
    "            continue\n",
    "\n",
    "        if parts[0] == clip_base:\n",
    "            try:\n",
    "                start = float(parts[3])\n",
    "                end = float(parts[4])\n",
    "                return end - start\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9d9e87-183b-4f70-b2a2-5d9c5b152e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(model_id):\n",
    "    \"\"\"Create a pipeline for automatic speech recognition using the specified model.\"\"\"\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "        return_timestamps=\"word\"\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a2df0-036f-46f1-9b08-30d41be30d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clips_with_models_ctm(models):\n",
    "    for model_path in models:\n",
    "        pipe = make_pipeline(model_path)\n",
    "        model_name = model_path.split('/')[-1]\n",
    "        counter = 0\n",
    "\n",
    "        # Set up output directories\n",
    "        model_dir = Path(\"results\") / model_name\n",
    "        ctm_dir = model_dir / \"ctm\"\n",
    "        tsv_dir = model_dir / \"tsv\"\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ctm_dir.mkdir(exist_ok=True)\n",
    "        tsv_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Progress tracking\n",
    "        progress_file = model_dir / \"progress.tsv\"\n",
    "        completed_folders = set()\n",
    "        if progress_file.exists():\n",
    "            with open(progress_file, \"r\") as pf:\n",
    "                completed_folders = set(line.strip() for line in pf)\n",
    "\n",
    "        folders = list_folders()\n",
    "\n",
    "        for folder in tqdm(folders, desc=f\"{model_name} - folders\", unit=\"folder\", dynamic_ncols=True, position=0):\n",
    "            if folder in completed_folders:\n",
    "                continue\n",
    "\n",
    "            x = folder.rstrip('/').split('.')[0]\n",
    "            files = list_s3_files_folder(folder)\n",
    "\n",
    "            ctm_lines = []\n",
    "            timing_lines = []\n",
    "            file_sentences = {}\n",
    "\n",
    "            ref_file = get_reference(folder)\n",
    "\n",
    "            for file in tqdm(files, desc=f\"Â  {folder}\", unit=\"file\", leave=False, dynamic_ncols=True, position=1):\n",
    "                counter += 1\n",
    "                audio_file = get_file_from_folder(folder, file)\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "                result = pipe(audio_file, generate_kwargs={\"language\": \"dutch\"}) ## set to english for primock\n",
    "                end_time = time.perf_counter()\n",
    "\n",
    "                file_id = Path(file).stem\n",
    "\n",
    "                chunks = result.get(\"chunks\", [])\n",
    "                words = []\n",
    "                for i, word_data in enumerate(chunks):\n",
    "                    start = float(word_data[\"timestamp\"][0])\n",
    "                    end = word_data[\"timestamp\"][1]\n",
    "\n",
    "                    if end is None:\n",
    "                        if i < len(chunks) - 1:\n",
    "                            end = float(chunks[i + 1][\"timestamp\"][0])\n",
    "                        else:\n",
    "                            end = float(get_duration(ref_file, file))\n",
    "\n",
    "                    duration = end - start\n",
    "                    word = word_data[\"text\"].strip()\n",
    "                    confidence = word_data.get(\"confidence\", 1.0)\n",
    "\n",
    "                    ctm_lines.append(f\"{file_id} 1 {start:.2f} {duration:.2f} {word} {confidence:.4f}\")\n",
    "                    words.append(word)\n",
    "\n",
    "                execution_time = end_time - start_time\n",
    "                duration = get_duration(ref_file, file)\n",
    "\n",
    "                rtf = f\"{(execution_time / duration):.4f}\"\n",
    "\n",
    "                prediction = \" \".join(words)\n",
    "                file_sentences[file_id] = prediction\n",
    "\n",
    "                timing_lines.append(f\"{file_id}\\t{rtf}\\t{prediction}\")\n",
    "\n",
    "            safe_folder_name = folder.split(\".\")[0]\n",
    "            ctm_filename = f\"{model_name}_{safe_folder_name}.ctm\"\n",
    "            tsv_filename = f\"{model_name}_{safe_folder_name}.tsv\"\n",
    "\n",
    "            with open(ctm_dir / ctm_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\".join(ctm_lines) + \"\\n\")\n",
    "\n",
    "            with open(tsv_dir / tsv_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"file\\tRTF\\tprediction\\n\")  # TSV header\n",
    "                f.write(\"\\n\".join(timing_lines) + \"\\n\")\n",
    "\n",
    "            # Mark this folder as completed\n",
    "            with open(progress_file, \"a\") as pf:\n",
    "                pf.write(folder + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc626ee-7a73-406d-bfa1-4ead7928170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = get_s3_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb8c01-362f-41c9-a21f-5881adb2977b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_to_test = [\"openai/whisper-tiny\", \"openai/whisper-small\", \"openai/whisper-base\", \n",
    "                  \"openai/whisper-medium\", \"openai/whisper-large-v3\", \"openai/whisper-large-v3-turbo\"]\n",
    "\n",
    "predict_clips_with_models_ctm(models_to_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
