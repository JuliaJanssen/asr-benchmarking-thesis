{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaabcb96",
   "metadata": {},
   "source": [
    "Please note: this code is adapted from https://github.com/SaraSun01/thesis_closed_and_opened_ASR_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc66664",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from rapidfuzz import process, fuzz\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_path = Path(r\"path/to/your/data/hyp\") ###\n",
    "ref_path = Path(r\"path/to/your/data/ref/7_reference.stm\") ###\n",
    "out_path = Path(r\"path/to/your/data/analysis/results/3. medical/nl\") ###\n",
    "snomed_path = Path(r\"path/to/your/data/analysis/results/3. medical/snomed_disease_terms_all.csv\") ###\n",
    " \n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc62bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stm_to_dict(stm_path):\n",
    "    stm_dict = defaultdict(list)\n",
    "    with open(stm_path, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split(maxsplit=6)\n",
    "            if len(parts) < 7:\n",
    "                continue\n",
    "            file_id = parts[0]\n",
    "            text = parts[6]\n",
    "            clean_text = re.sub(r'<[^>]+>', '', text).strip()\n",
    "            stm_dict[file_id].append(clean_text)\n",
    "    \n",
    "    return {k: ' '.join(v) for k, v in stm_dict.items()}\n",
    "\n",
    "def load_hyp(hyp_path):\n",
    "    df = pd.read_csv(hyp_path, sep=\"\\t\")\n",
    "    df = df.rename(columns={\"file\": \"transcript_id\"})\n",
    "    df = df.drop(columns=[\"RTF\"])\n",
    "    df = df.set_index(\"transcript_id\")\n",
    "    return df\n",
    "\n",
    "def get_transcript_id(utt_id):\n",
    "    parts = utt_id.split('_')\n",
    "    id = f\"{parts[1]}-{parts[2]}\"\n",
    "    return id\n",
    "\n",
    "def get_utt_code(utt_id):\n",
    "    parts = utt_id.split('_')\n",
    "    int(parts[3][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3cd70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===Tokenization + combination of 1-gram and 2-gram (including cleaning) ===\n",
    "def generate_candidates(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.is_alpha]\n",
    "    # Cleaning: Remove stopwords and words with length less than or equal to 3\n",
    "    tokens = [t for t in tokens if t.lower() not in stopwords and len(t) > 3]\n",
    "    one_grams = tokens\n",
    "    two_grams = [' '.join([tokens[i], tokens[i+1]]) for i in range(len(tokens)-1)]\n",
    "    return list(set(one_grams + two_grams))\n",
    "\n",
    "# === Perform fuzzy matching (keep only the top 3 matches) ===\n",
    "def fuzzy_match_terms(candidates, term_df, score_threshold=90):\n",
    "    term_list = term_df[\"term\"].tolist()\n",
    "    match_pool = []\n",
    "    for candidate in candidates:\n",
    "        match, score, idx = process.extractOne(\n",
    "            candidate, term_list, scorer=fuzz.ratio\n",
    "        )\n",
    "        if score >= score_threshold:\n",
    "            matched_row = term_df.iloc[idx]\n",
    "            match_pool.append({\n",
    "                \"candidate\": candidate,\n",
    "                \"matched_term\": match,\n",
    "                \"score\": score,\n",
    "                \"type\": matched_row[\"type\"],\n",
    "                \"conceptId\": matched_row[\"conceptId\"]\n",
    "            })\n",
    "    # Returns the top 3 matches with the highest scores\n",
    "    return sorted(match_pool, key=lambda x: x[\"score\"], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bccc63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7806c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_df = pd.read_csv(snomed_path)\n",
    "ref_dict = parse_stm_to_dict(ref_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c416845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ref(ref_dict):\n",
    "    # === Main loop: utterance-by-utterance matching ===\n",
    "    all_matches = []\n",
    "\n",
    "    for uid, text in ref_dict.items():\n",
    "        candidates = generate_candidates(text)\n",
    "        clinical_finding_matches = fuzzy_match_terms(candidates, snomed_df)\n",
    "        for m in clinical_finding_matches:\n",
    "            m[\"transcript_id\"] = uid\n",
    "        all_matches.extend(clinical_finding_matches)\n",
    "\n",
    "    # === save output ===\n",
    "    matched_df = pd.DataFrame(all_matches)\n",
    "    matched_df.to_csv(out_path / \"matched_results.csv\", index=False)\n",
    "    print(f\"Finished! Saved as {out_path / 'matched_results.csv'}\")\n",
    "    return matched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2667c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECT_THRESHOLD = 90\n",
    "SUBSTITUTION_THRESHOLD = 70\n",
    "MAX_NGRAM = 3\n",
    "\n",
    "def generate_ngrams(tokens, max_n=3):\n",
    "    ngrams = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ngrams += [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a2def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished! Saved as C:\\Users\\Topicus\\Documents\\Datasets\\analysis\\results\\3. medical\\nl\\matched_results.csv\n"
     ]
    }
   ],
   "source": [
    "load_ref(ref_dict)\n",
    "ref_dict_path = Path(r\"path/to/your/data/3. medical/nl/matched_results.csv\") ###\n",
    "with open(ref_dict_path, 'r', encoding='utf-8') as f:\n",
    "    ref_dict = pd.read_csv(f)\n",
    "ref_df = ref_dict.set_index('transcript_id')[['matched_term']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98b7a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_matches(aligned_df):\n",
    "    results = []\n",
    "    for _, row in aligned_df.iterrows():\n",
    "        uid = row.name # transcript_id\n",
    "        hyp_raw = str(row['prediction'])\n",
    "        hyp_tokens = hyp_raw.split()\n",
    "        hyp_ngrams = generate_ngrams(hyp_tokens, MAX_NGRAM)\n",
    "        matched_term = row['matched_term']\n",
    "\n",
    "        best_match, score, _ = process.extractOne(matched_term, hyp_ngrams, scorer=fuzz.ratio)\n",
    "        \n",
    "        if score >= CORRECT_THRESHOLD:\n",
    "            match_type = \"correct\"\n",
    "        elif score >= SUBSTITUTION_THRESHOLD:\n",
    "            match_type = \"substitution\"\n",
    "        else:\n",
    "            match_type = \"deletion\"\n",
    "            best_match = \"\"\n",
    "\n",
    "        results.append({\n",
    "            \"id\": uid,\n",
    "            \"matched_term\": matched_term,\n",
    "            \"matched_hyp_phrase\": best_match,\n",
    "            \"match_score\": round(score, 2),\n",
    "            \"match_type\": match_type\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96726d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyp_matches(hyp_df):\n",
    "    unmatched_hyp_df = hyp_df[~hyp_df.index.isin(ref_df.index)]\n",
    "    unmatched_hyp_df = unmatched_hyp_df['prediction'].to_dict()\n",
    "    insertion_candidates = load_ref(unmatched_hyp_df)\n",
    "    print(f\"Unmatched Hypothesis for {hyp_df.index[0]}: {unmatched_hyp_df}\")\n",
    "    print(f\"Insertion Candidates for {hyp_df.index[0]}: {insertion_candidates}\")\n",
    "    return insertion_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d811221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-transcribe: 0.0781\n",
      "wav2vec2-dutch-large-ft-cgn: 0.4453\n",
      "wav2vec2-large-xlsr-53-dutch: 0.5430\n",
      "whisper-base: 0.6094\n",
      "whisper-large-v3: 0.1484\n",
      "whisper-large-v3-turbo: 0.1367\n",
      "whisper-medium: 0.2734\n",
      "whisper-small: 0.4102\n",
      "whisper-tiny: 0.7891\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for subfolder in os.listdir(hyp_path):\n",
    "    subfolder_path = os.path.join(hyp_path, subfolder, \"tsv\")\n",
    "    \n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(subfolder_path):\n",
    "        if filename.endswith(\"_7.tsv\"):\n",
    "            hyp_df = load_hyp(os.path.join(subfolder_path, filename))\n",
    "\n",
    "            #print(get_hyp_matches(hyp_df))\n",
    "\n",
    "            aligned_df = pd.merge(ref_df, hyp_df, on='transcript_id')\n",
    "            aligned_df['matched_term'] = aligned_df['matched_term'].apply(clean_text)\n",
    "            aligned_df['prediction'] = aligned_df['prediction'].apply(clean_text)\n",
    "\n",
    "            results = analyze_matches(aligned_df)\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.to_csv(os.path.join(out_path, f\"{subfolder}_mcwer_nl.csv\"), index=False)\n",
    "\n",
    "            S = (results_df[\"match_type\"] == \"substitution\").sum()\n",
    "            D = (results_df[\"match_type\"] == \"deletion\").sum()\n",
    "            N = len(results_df)\n",
    "\n",
    "            mwer = (S + D) / N if N > 0 else 0\n",
    "            print(f\"{subfolder}: {mwer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d7696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
