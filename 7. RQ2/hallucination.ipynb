{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0110a9ea",
   "metadata": {},
   "source": [
    "Please note: this uses the opentaal-wordlist which you can find here https://github.com/OpenTaal/opentaal-wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53351029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58443d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_path = Path(\"path/to/your/data/analysis/hyp\") ###\n",
    "ref_path = Path(\"path/to/your/data/analysis/ref\") ###\n",
    "out_path = Path(\"path/to/your/data/analysis/results/2. hallucination/raw\") ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fddc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 199404 Dutch words\n"
     ]
    }
   ],
   "source": [
    "with open(\"basiswoorden-gekeurd.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dutch_words = [line.strip() for line in f if line.strip()]\n",
    "print(f\"Loaded {len(dutch_words)} Dutch words\")\n",
    "\n",
    "transformer_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207699b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_embeddings = transformer_model.encode(dutch_words, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the embeddings to a file\n",
    "embeddings_np = dict_embeddings.cpu().numpy()\n",
    "np.save(\"dutch_embeddings.npy\", embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a899e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to load the embedding model if you've generated it before\n",
    "#dict_embeddings = np.load(\"dutch_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nonsense_word(word, threshold=0.95):\n",
    "    word_embedding = transformer_model.encode(word, convert_to_tensor=True, show_progress_bar=False)\n",
    "    cosine_scores = util.cos_sim(word_embedding, dict_embeddings)\n",
    "    max_score = cosine_scores.max().item()\n",
    "    return max_score < threshold\n",
    "\n",
    "def detect_nonsense_words(text):\n",
    "    nonsense_words = []\n",
    "    for word in text.split():\n",
    "        if is_nonsense_word(word):\n",
    "            nonsense_words.append(word)\n",
    "\n",
    "    total_chars = sum(len(word) for word in text.split())\n",
    "    nonsense_chars = sum(len(word) for word in text.split() if word in nonsense_words)\n",
    "\n",
    "    if nonsense_chars > 0 and total_chars > 0:\n",
    "        percentage_nonsense = nonsense_chars / total_chars\n",
    "        return percentage_nonsense\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_repeated_sequence(text, min_repeats=3, max_phrase_len=5):\n",
    "    #detect if any sequence of words (length 1 to max_phrase_len) repeats min_repeats times consecutively\n",
    "\n",
    "    text = ' '.join(text.lower().split())\n",
    "\n",
    "    for length in range(1, max_phrase_len + 1):\n",
    "        #   (\\b(?:\\w+\\s+){length-1}\\w+\\b)   captures a phrase of 'length' words\n",
    "        #   (?:\\s+\\1){min_repeats,}         matches that phrase repeated at least (min_repeats) more times\n",
    "        pattern = rf'(\\b(?:\\w+\\s+){{{length-1}}}\\w+\\b)(?:\\s+\\1){{{min_repeats},}}'\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491abeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stm_file(stm_path):\n",
    "    #parses stm file and returns a list of (id, reference_text)\n",
    "\n",
    "    references = []\n",
    "    with open(stm_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=6)\n",
    "            utt_id = parts[0]  \n",
    "            text = parts[6] if len(parts) > 6 else None\n",
    "            references.append((utt_id, text))\n",
    "    return references\n",
    "\n",
    "NON_SPEECH_TOKENS = {\n",
    "    \"muziek\",\n",
    "    \"gelach\",\n",
    "    \"zang en muziek\",\n",
    "    \"geluid van machines\",\n",
    "    \"ze zucht\",\n",
    "    \"hij zucht\",\n",
    "    \"geluid van klaxon\",\n",
    "    \"applaus\",\n",
    "    \"geluid\",\n",
    "    \"ruis\",\n",
    "    \"kuch\",\n",
    "    \"niezen\",\n",
    "    \"stilte\",\n",
    "    \"onverstaanbaar\",\n",
    "    \"geluid van verkeer\"\n",
    "}\n",
    "\n",
    "def cosine_similarity(id, ref_list, prediction):\n",
    "        ref_text = ref_list.get(id)\n",
    "        if ref_text is None and (prediction != \"\" and prediction not in NON_SPEECH_TOKENS):\n",
    "            return True #if no reference text, consider it a hallucination automatically\n",
    "        elif ref_text is None and (prediction == \"\" or prediction in NON_SPEECH_TOKENS):\n",
    "            return False #correctly identified silence\n",
    "\n",
    "        ref_vec = transformer_model.encode(ref_text, convert_to_tensor=True)\n",
    "        hyp_vec = transformer_model.encode(prediction, convert_to_tensor=True)\n",
    "        score = util.cos_sim(ref_vec, hyp_vec).item()\n",
    "        score = round(score, 3)\n",
    "        #print(f\"Cosine similarity for {ref_text}, {prediction}: {score}\")\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951945bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tsv(input_path, output_path):\n",
    "    with open(input_path, newline='', encoding=\"utf-8\") as infile, \\\n",
    "     open(output_path, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "\n",
    "        reader = csv.reader(infile, delimiter=\"\\t\")\n",
    "        writer = csv.writer(outfile, delimiter=\"\\t\")\n",
    "\n",
    "        subset = input_path.stem.split(\"_\")[1]\n",
    "        ref_list = dict(parse_stm_file(ref_path / f\"{subset}_reference.stm\"))\n",
    "        num_lines = sum(1 for _ in open(input_path, encoding=\"utf-8\")) - 1\n",
    "        \n",
    "        writer.writerow([\"file\", \"nonsense_words\", \"repeating_words\", \"incorrect_sentence_meaning\", \"prediction\"])\n",
    "        next(reader)\n",
    "\n",
    "        for row in tqdm(reader, total=num_lines, desc=\"Processing rows\", unit=\"row\"):\n",
    "            if len(row) < 3:\n",
    "                writer.writerow([row[0], \"\", \"\", \"\", \"\"])\n",
    "                continue\n",
    "\n",
    "            file_id = row[0]\n",
    "            prediction = row[2]\n",
    "            prediction = prediction.strip().lower()\n",
    "            prediction = re.sub(r'[0-9]+', '', prediction)  #remove numbers\n",
    "            prediction = re.sub(r'[^\\w\\s]', '', prediction)  #remove punctuation\n",
    "            \n",
    "            nonsense = detect_nonsense_words(prediction)\n",
    "            repeating = has_repeated_sequence(prediction)\n",
    "            similarity = cosine_similarity(file_id, ref_list, prediction)\n",
    "\n",
    "            writer.writerow([file_id, str(nonsense), str(repeating), str(similarity), prediction])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfdaf721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\Topicus\\Documents\\Datasets\\analysis\\hyp\\whisper-small\\tsv\\whisper-small_6.tsv to C:\\Users\\Topicus\\Documents\\Datasets\\analysis\\results\\2. hallucination\\raw\\whisper-small_6_hallucination.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 2472/2472 [18:59:59<00:00, 27.67s/row]        \n"
     ]
    }
   ],
   "source": [
    "model_names = [f.name for f in hyp_path.iterdir() if f.is_dir()]\n",
    "\n",
    "for model in model_names:\n",
    "    model_path = hyp_path / model\n",
    "    ctm_path = model_path / \"tsv\"\n",
    "\n",
    "    for hyp_file in list(ctm_path.glob(\"*.tsv\")):\n",
    "        subset = hyp_file.stem.split(\"_\")[1]\n",
    "        if int(subset) == 5 or int(subset) == 6:\n",
    "            out_file = out_path / f\"{model}_{subset}_hallucination.tsv\"\n",
    "            if out_file.exists():\n",
    "                continue\n",
    "            print(f\"Processing {hyp_file} to {out_file}\")\n",
    "            check_tsv(hyp_file, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
